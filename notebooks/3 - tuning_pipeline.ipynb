{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which models are performing better, it's time to perform cross validation and tune hyperparameters.\n",
    "- Grid search is a great method for checking off both of these tasks.\n",
    "- Do a google search for hyperparameter ranges for each type of model.\n",
    "- Check out RandomizedSearchCV for faster computation with large grids.\n",
    "- If you have access to a GPU, xgboost can make use of it, but requires additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform tuning and cross validation here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cities = pd.read_csv('../data/cities.csv')\n",
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv')\n",
    "\n",
    "CV_df = pd.concat([cities, X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_df.drop(columns= 'city_mean_sold_price', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_validation(train_df, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    train_df_copy = train_df.copy()\n",
    "    \n",
    "    training_folds = []\n",
    "    validation_folds = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(train_df_copy):\n",
    "        train_fold, val_fold = train_df_copy.iloc[train_index], train_df_copy.iloc[val_index]\n",
    "\n",
    "        city_mean_train = train_fold.groupby('location.address.city')['description.sold_price'].mean()\n",
    "\n",
    "        # Merge mean price into both training and validation folds\n",
    "        train_fold = train_fold.merge(city_mean_train, left_on='location.address.city', right_index=True, how='left', suffixes=('', '_city_mean'))\n",
    "        val_fold = val_fold.merge(city_mean_train, left_on='location.address.city', right_index=True, how='left', suffixes=('', '_city_mean'))\n",
    "\n",
    "        # Fill missing values in both folds with global mean sold price\n",
    "        global_mean = train_df_copy['description.sold_price'].mean()\n",
    "        train_fold['description.sold_price_city_mean'].fillna(global_mean, inplace=True)\n",
    "        val_fold['description.sold_price_city_mean'].fillna(global_mean, inplace=True)\n",
    "\n",
    "        # Drop the city column from both folds\n",
    "        train_fold.drop(columns=['location.address.city'], inplace=True)\n",
    "        val_fold.drop(columns=['location.address.city'], inplace=True)\n",
    "\n",
    "        training_folds.append(train_fold)\n",
    "        validation_folds.append(val_fold)\n",
    "\n",
    "    return training_folds, validation_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds, validation_folds = custom_cross_validation(CV_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3363, 57)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_folds[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 57)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_folds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 does not contain any NaN values.\n",
      "Training fold 2 does not contain any NaN values.\n",
      "Training fold 3 does not contain any NaN values.\n",
      "Training fold 4 does not contain any NaN values.\n",
      "Training fold 5 does not contain any NaN values.\n"
     ]
    }
   ],
   "source": [
    "for i, train_fold in enumerate(training_folds):\n",
    "    nan_indices = train_fold.isnull().any(axis=1)\n",
    "    if nan_indices.any():\n",
    "        print(f\"Training fold {i+1} contains NaN values.\")\n",
    "        print(train_fold[nan_indices])\n",
    "    else:\n",
    "        print(f\"Training fold {i+1} does not contain any NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def hyperparameter_search(training_folds, validation_folds, param_grid):\n",
    "    all_r2_scores = []\n",
    "    all_best_params_list = []\n",
    "\n",
    "    for params in itertools.product(*param_grid.values()):\n",
    "        r2_scores = []\n",
    "        best_params_list = []\n",
    "\n",
    "        for train_fold, val_fold in zip(training_folds, validation_folds):\n",
    "            rf = RandomForestRegressor(**dict(zip(param_grid.keys(), params)))\n",
    "\n",
    "            X_train_fold = train_fold.drop(columns=['description.sold_price'])\n",
    "            y_train_fold = train_fold['description.sold_price']\n",
    "            X_val_fold = val_fold.drop(columns=['description.sold_price'])\n",
    "            y_val_fold = val_fold['description.sold_price']\n",
    "\n",
    "            rf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            r2_score = rf.score(X_val_fold, y_val_fold)\n",
    "\n",
    "            r2_scores.append(r2_score)\n",
    "            best_params_list.append(params)\n",
    "\n",
    "        all_r2_scores.append(r2_scores)\n",
    "        all_best_params_list.append(best_params_list)\n",
    "\n",
    "    avg_r2_scores = np.mean(all_r2_scores, axis=1)\n",
    "    best_params_idx = np.argmax(avg_r2_scores)\n",
    "    best_params = all_best_params_list[best_params_idx][0]  \n",
    "\n",
    "    return avg_r2_scores, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_r2_scores, best_params = hyperparameter_search(training_folds, validation_folds, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that we save our models.  In the old days, one just simply pickled (serialized) the model.  Now, however, certain model types have their own save format.  If the model is from sklearn, it can be pickled, if it's xgboost, for example, the newest format to save it in is JSON, but it can also be pickled.  It's a good idea to stay with the most current methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9958753792639732\n"
     ]
    }
   ],
   "source": [
    "best_model = RandomForestRegressor(n_estimators= 100, max_depth= 30, \n",
    "                                   min_samples_split= 2, min_samples_leaf= 1, \n",
    "                                   max_features= 'sqrt').fit(X_train, np.array(y_train).ravel())\n",
    "\n",
    "print(best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../models/tuned_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've identified which model works the best, implement a pipeline to make sure that you haven't leaked any data, and that the model could be easily deployed if desired.\n",
    "- Beware that a pipeline can only handle functions with fit and transform methods.\n",
    "- Classes can be used to get around this, but now sklearn has a wrapper for user defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline \n",
    "from functions_variables import *\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "directory = '../data'\n",
    "train = pd.read_csv('../data/X_train.csv')\n",
    "columns_to_keep = list(train.columns)\n",
    "\n",
    "columns_to_drop_na = ['description.type', 'description.year_built',\n",
    "                      'description.lot_sqft', 'description.sqft', 'location.address.coordinate.lon',\n",
    "                      'location.address.coordinate.lat']\n",
    "\n",
    "columns_fill_values = {\n",
    "    'description.baths_3qtr': 0,\n",
    "    'description.baths_full': 0,\n",
    "    'description.baths_half': 0,\n",
    "    'description.baths': 0,\n",
    "    'description.garage': 0,\n",
    "    'description.beds': 0,\n",
    "    'description.sub_type': 'N/A',\n",
    "    'location.address.city': 'N/A',\n",
    "    'description.stories': 1\n",
    "}\n",
    "\n",
    "columns_to_log = ['description.lot_sqft', 'description.sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransform(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Drop rows with zero values in specified columns\n",
    "        for col in self.columns:\n",
    "            X_transformed = X_transformed[X_transformed[col] != 0]\n",
    "\n",
    "        # Apply logarithmic transformation\n",
    "        for col in self.columns:\n",
    "            X_transformed[col] = np.log(X_transformed[col])\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('get_dataframe', FunctionTransformer(get_dataframe)),\n",
    "    ('encode_tags', TagsEncoder()),\n",
    "    ('drop_NAs', DropMissingValues(columns=columns_to_drop_na)),\n",
    "    ('fill_NAs', FillMissingValues(fill_values_dict=columns_fill_values) ),\n",
    "    ('transform_types', TypeTransformer()),\n",
    "    ('merge_city_means', MergeAndImputeTransformer('../data/city_means.csv')),\n",
    "    ('log_transform', LogTransform(columns_to_log)),\n",
    "   ('select_columns', ColumnSelector(columns_to_keep)),\n",
    "   ('scale', PretrainedMinMaxScale('../models/scaler.pkl')),\n",
    "    ('predict', PredictionsFromModel('../models/tuned_model.pkl'))\n",
    "])\n",
    "\n",
    "df, pred = pipeline.fit_transform(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6354, 56)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6354,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines come from sklearn.  When a pipeline is pickled, all of the information in the pipeline is stored with it.  For example, if we were deploying a model, and we had fit a scaler on the training data, we would want the same, already fitted scaling object to transform the new data with.  This is all stored when the pipeline is pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your pipeline here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
